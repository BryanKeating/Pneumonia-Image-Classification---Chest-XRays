{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following along with CNN - Codealong\n",
    "#for create new folders and move the new files in there\n",
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataAug = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range = 45,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range = 0.2, # Randomly zoom image \n",
    "    width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip = True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 5216 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# get all the data in the directory xray/test (624 images), and reshape them\n",
    "test_generator_base = ImageDataGenerator(rescale= 1./255).flow_from_directory(\"Data/chest_xray/chest_xray/test/\",\n",
    "                                                                         batch_size = 624)\n",
    "\n",
    "# get all the data in the directory xray/val (16 images), and reshape them\n",
    "val_generator_base = ImageDataGenerator(1./255).flow_from_directory(\"Data/chest_xray/chest_xray/val/\",\n",
    "                                                               batch_size = 16)\n",
    "\n",
    "# get all the data in the directory xray/train (5216 images), and reshape them\n",
    "train_generator_base = ImageDataGenerator(1./255).flow_from_directory(\"Data/chest_xray/chest_xray/train/\",\n",
    "                                              batch_size = 5216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = next(train_generator_base)\n",
    "\n",
    "test_images, test_labels = next(test_generator_base)\n",
    "\n",
    "val_images, val_labels = next(val_generator_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5216"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_aug = data_aug()\n",
    "# data_aug.fit(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 5216\n",
      "Number of testing samples: 624\n",
      "Number of validation samples: 16\n",
      "train_images shape: (5216, 256, 256, 3)\n",
      "train_labels shape: (5216, 2)\n",
      "test_images shape: (624, 256, 256, 3)\n",
      "test_labels shape: (624, 2)\n",
      "val_images shape: (16, 256, 256, 3)\n",
      "val_labels shape: (16, 2)\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset again\n",
    "m_train = train_images.shape[0]\n",
    "num_px = train_images.shape[1]\n",
    "m_test = test_images.shape[0]\n",
    "m_val = val_images.shape[0]\n",
    "\n",
    "print (\"Number of training samples: \" + str(m_train))\n",
    "print (\"Number of testing samples: \" + str(m_test))\n",
    "print (\"Number of validation samples: \" + str(m_val))\n",
    "print (\"train_images shape: \" + str(train_images.shape))\n",
    "print (\"train_labels shape: \" + str(train_labels.shape))\n",
    "print (\"test_images shape: \" + str(test_images.shape))\n",
    "print (\"test_labels shape: \" + str(test_labels.shape))\n",
    "print (\"val_images shape: \" + str(val_images.shape))\n",
    "print (\"val_labels shape: \" + str(val_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 5216 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# get all the data in the directory xray/test (624 images), and reshape them\n",
    "test_generator = DataAug.flow_from_directory(\"Data/chest_xray/chest_xray/test/\", \n",
    "                                                                              batch_size = 624, \n",
    "                                                                             color_mode= \"grayscale\")\n",
    "\n",
    "# get all the data in the directory xray/val (16 images), and reshape them\n",
    "val_generator = DataAug.flow_from_directory(\"Data/chest_xray/chest_xray/val/\", \n",
    "                                                                    batch_size = 16,\n",
    "                                                                   color_mode= \"grayscale\")\n",
    "\n",
    "# get all the data in the directory xray/train (5216 images), and reshape them\n",
    "train_generator = DataAug.flow_from_directory(\"Data/chest_xray/chest_xray/train/\", \n",
    "                                                                      batch_size = 5216, \n",
    "                                                                     color_mode= \"grayscale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = next(train_generator)\n",
    "\n",
    "test_images, test_labels = next(test_generator)\n",
    "\n",
    "val_images, val_labels = next(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 5216\n",
      "Number of testing samples: 624\n",
      "Number of validation samples: 16\n",
      "train_images shape: (5216, 256, 256, 1)\n",
      "train_labels shape: (5216, 2)\n",
      "test_images shape: (624, 256, 256, 1)\n",
      "test_labels shape: (624, 2)\n",
      "val_images shape: (16, 256, 256, 1)\n",
      "val_labels shape: (16, 2)\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset again\n",
    "m_train = train_images.shape[0]\n",
    "num_px = train_images.shape[1]\n",
    "m_test = test_images.shape[0]\n",
    "m_val = val_images.shape[0]\n",
    "\n",
    "print (\"Number of training samples: \" + str(m_train))\n",
    "print (\"Number of testing samples: \" + str(m_test))\n",
    "print (\"Number of validation samples: \" + str(m_val))\n",
    "print (\"train_images shape: \" + str(train_images.shape))\n",
    "print (\"train_labels shape: \" + str(train_labels.shape))\n",
    "print (\"test_images shape: \" + str(test_images.shape))\n",
    "print (\"test_labels shape: \" + str(test_labels.shape))\n",
    "print (\"val_images shape: \" + str(val_images.shape))\n",
    "print (\"val_labels shape: \" + str(val_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5216, 65536)\n",
      "(624, 65536)\n",
      "(16, 65536)\n"
     ]
    }
   ],
   "source": [
    "#reshaping data\n",
    "train_img = train_images.reshape(train_images.shape[0], -1)\n",
    "test_img = test_images.reshape(test_images.shape[0], -1)\n",
    "val_img = val_images.reshape(val_images.shape[0], -1)\n",
    "\n",
    "print(train_img.shape)\n",
    "print(test_img.shape)\n",
    "print(val_img.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping label\n",
    "train_y = np.reshape(train_labels[:,0], (5216,1))\n",
    "test_y = np.reshape(test_labels[:,0], (624,1))\n",
    "val_y = np.reshape(val_labels[:,0], (16,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5216, 1)\n",
      "(624, 1)\n",
      "(16, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)\n",
    "print(val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5216, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "l = []\n",
    "for i in train_img:\n",
    "    if(i[1] == 0):\n",
    "        l.append(\"Pneumonia\")\n",
    "    else:\n",
    "        l.append(\"Normal\")\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.countplot(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_img, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6778846153846154"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_img,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix, accuracy_score, confusion_matrix, plot_roc_curve, \\\n",
    "make_scorer, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2905982905982906"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(test_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf= RandomForestClassifier(criterion = \"entropy\", max_depth = 15,\n",
    "                          min_samples_split= 5, n_estimators= 700,\n",
    "                          max_features = 'log2', random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=15, max_features='log2',\n",
       "                       min_samples_split=5, n_estimators=700, random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(train_img, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.768     , 0.72      , 0.76      , 0.728     , 0.78225806])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(rf, test_img, test_y, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score of random forest model: 0.7291666666666666\n"
     ]
    }
   ],
   "source": [
    "print(f'Test score of random forest model: {rf.score(test_img, test_y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 20)                1310740   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 147       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 40        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 1,310,933\n",
      "Trainable params: 1,310,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#baseline model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(20, activation='relu', input_shape=(65536,))) # 2 hidden layers\n",
    "model.add(layers.Dense(7, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 627.4537 - accuracy: 0.7387 - val_loss: 0.7066 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.6104 - accuracy: 0.7429 - val_loss: 0.7298 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5898 - accuracy: 0.7429 - val_loss: 0.7522 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5799 - accuracy: 0.7429 - val_loss: 0.7708 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5751 - accuracy: 0.7429 - val_loss: 0.7852 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5726 - accuracy: 0.7429 - val_loss: 0.7963 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5714 - accuracy: 0.7429 - val_loss: 0.8050 - val_accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5707 - accuracy: 0.7429 - val_loss: 0.8110 - val_accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5704 - accuracy: 0.7429 - val_loss: 0.8154 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5702 - accuracy: 0.7429 - val_loss: 0.8188 - val_accuracy: 0.5000\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5701 - accuracy: 0.7429 - val_loss: 0.8210 - val_accuracy: 0.5000\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5701 - accuracy: 0.7429 - val_loss: 0.8230 - val_accuracy: 0.5000\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5701 - accuracy: 0.7429 - val_loss: 0.8243 - val_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8254 - val_accuracy: 0.5000\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - ETA: 0s - loss: 0.5699 - accuracy: 0.74 - 1s 7ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8264 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8269 - val_accuracy: 0.5000\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8272 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8274 - val_accuracy: 0.5000\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8273 - val_accuracy: 0.5000\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8276 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8278 - val_accuracy: 0.5000\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8273 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8275 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8279 - val_accuracy: 0.5000\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8276 - val_accuracy: 0.5000\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8277 - val_accuracy: 0.5000\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8278 - val_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8279 - val_accuracy: 0.5000\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8278 - val_accuracy: 0.5000\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8280 - val_accuracy: 0.5000\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8279 - val_accuracy: 0.5000\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8280 - val_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8281 - val_accuracy: 0.5000\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8280 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8279 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8280 - val_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8280 - val_accuracy: 0.5000\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8275 - val_accuracy: 0.5000\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8275 - val_accuracy: 0.5000\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8273 - val_accuracy: 0.5000\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8274 - val_accuracy: 0.5000\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8273 - val_accuracy: 0.5000\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8275 - val_accuracy: 0.5000\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8274 - val_accuracy: 0.5000\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8277 - val_accuracy: 0.5000\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8275 - val_accuracy: 0.5000\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8277 - val_accuracy: 0.5000\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8280 - val_accuracy: 0.5000\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8280 - val_accuracy: 0.5000\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 0.5700 - accuracy: 0.7429 - val_loss: 0.8280 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics =  ['accuracy'])\n",
    "\n",
    "histoire = model.fit(train_img,\n",
    "                    train_y,\n",
    "                    epochs = 50,\n",
    "                    batch_size = 32,\n",
    "                    validation_data=(val_img,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 1s 3ms/step - loss: 0.5700 - accuracy: 0.7429\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_img, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 3ms/step - loss: 0.6952 - accuracy: 0.6250\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(test_img,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5699953436851501, 0.7429064512252808]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6952381730079651, 0.625]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-34-a38c7136417d>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_classes = model.predict_classes(test_img, verbose = 0)\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    "recall_score(test_y, yhat_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      16416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4194368   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,231,713\n",
      "Trainable params: 4,231,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras import regularizers\n",
    "\n",
    "# CNN build\n",
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', \n",
    "                        padding = \"same\",\n",
    "                       input_shape = (256,256,1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (4, 4), activation='relu', \n",
    "                       padding = 'same'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu',\n",
    "                       padding= 'same'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20/20 [==============================] - 11s 565ms/step - loss: nan - accuracy: 0.6625 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 11s 548ms/step - loss: nan - accuracy: 0.7219 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 11s 557ms/step - loss: nan - accuracy: 0.7000 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 11s 564ms/step - loss: nan - accuracy: 0.7656 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 11s 554ms/step - loss: nan - accuracy: 0.7437 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 11s 561ms/step - loss: nan - accuracy: 0.7063 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 11s 556ms/step - loss: nan - accuracy: 0.7219 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 12s 581ms/step - loss: nan - accuracy: 0.7437 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 10s 524ms/step - loss: nan - accuracy: 0.7750 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 11s 567ms/step - loss: nan - accuracy: 0.7781 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 11s 562ms/step - loss: nan - accuracy: 0.7344 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 11s 554ms/step - loss: nan - accuracy: 0.7188 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 11s 551ms/step - loss: nan - accuracy: 0.7656 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 12s 577ms/step - loss: nan - accuracy: 0.7563 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 11s 560ms/step - loss: nan - accuracy: 0.7688 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 11s 564ms/step - loss: nan - accuracy: 0.7156 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 11s 560ms/step - loss: nan - accuracy: 0.7656 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 11s 561ms/step - loss: nan - accuracy: 0.7219 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 10s 522ms/step - loss: nan - accuracy: 0.7500 - val_loss: nan - val_accuracy: 0.5000\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 11s 552ms/step - loss: nan - accuracy: 0.7563 - val_loss: nan - val_accuracy: 0.5000\n",
      "163/163 [==============================] - 33s 204ms/step - loss: nan - accuracy: 0.7429\n",
      "Training Score of first convolution neural network: [nan, 0.7429064512252808]\n",
      "20/20 [==============================] - 3s 173ms/step - loss: nan - accuracy: 0.6250\n",
      "Test Score of first convolution neural network: [nan, 0.625]\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    steps_per_epoch= 20,\n",
    "                    epochs=20,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(val_images, val_y))\n",
    "print(f\"Training Score of first convolution neural network: {model.evaluate(train_images, train_y)}\")\n",
    "print(f\"Test Score of first convolution neural network: {model.evaluate(test_images, test_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_classes = model.predict_classes(test_images, verbose = 0)\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    "recall_score(test_y, yhat_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      16416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4194368   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,231,713\n",
      "Trainable params: 4,231,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50/50 [==============================] - 38s 768ms/step - loss: 0.9782 - acc: 0.7250 - val_loss: 0.6980 - val_acc: 0.7500\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 34s 676ms/step - loss: 0.4519 - acc: 0.8125 - val_loss: 0.4884 - val_acc: 0.8125\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 37s 748ms/step - loss: 0.3756 - acc: 0.8200 - val_loss: 0.6503 - val_acc: 0.7500\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 34s 682ms/step - loss: 0.3076 - acc: 0.8625 - val_loss: 0.6813 - val_acc: 0.8125\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 34s 672ms/step - loss: 0.3281 - acc: 0.8550 - val_loss: 0.5329 - val_acc: 0.7500\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 34s 679ms/step - loss: 0.3366 - acc: 0.8450 - val_loss: 1.0012 - val_acc: 0.5625\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - 37s 748ms/step - loss: 0.3492 - acc: 0.8675 - val_loss: 0.8159 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "50/50 [==============================] - 39s 777ms/step - loss: 0.2850 - acc: 0.8875 - val_loss: 0.6590 - val_acc: 0.6875\n",
      "Epoch 9/20\n",
      "50/50 [==============================] - 34s 679ms/step - loss: 0.2683 - acc: 0.9225 - val_loss: 1.2831 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "50/50 [==============================] - 34s 681ms/step - loss: 0.3023 - acc: 0.8550 - val_loss: 0.6479 - val_acc: 0.7500\n",
      "Epoch 11/20\n",
      "50/50 [==============================] - 33s 668ms/step - loss: 0.2619 - acc: 0.8875 - val_loss: 0.8317 - val_acc: 0.6250\n",
      "Epoch 12/20\n",
      "50/50 [==============================] - 33s 656ms/step - loss: 0.2386 - acc: 0.8875 - val_loss: 1.0664 - val_acc: 0.6875\n",
      "Epoch 13/20\n",
      "50/50 [==============================] - 32s 642ms/step - loss: 0.2820 - acc: 0.8800 - val_loss: 0.8328 - val_acc: 0.6875\n",
      "Epoch 14/20\n",
      "50/50 [==============================] - 33s 657ms/step - loss: 0.2460 - acc: 0.8875 - val_loss: 1.4627 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "50/50 [==============================] - 33s 661ms/step - loss: 0.2057 - acc: 0.9200 - val_loss: 2.4157 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "50/50 [==============================] - 33s 656ms/step - loss: 0.2572 - acc: 0.9025 - val_loss: 0.7424 - val_acc: 0.8750\n",
      "Epoch 17/20\n",
      "50/50 [==============================] - 33s 657ms/step - loss: 0.1999 - acc: 0.9100 - val_loss: 1.3318 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "50/50 [==============================] - 33s 659ms/step - loss: 0.2713 - acc: 0.8925 - val_loss: 0.6477 - val_acc: 0.6250\n",
      "Epoch 19/20\n",
      "50/50 [==============================] - 39s 770ms/step - loss: 0.2236 - acc: 0.9150 - val_loss: 4.2651 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "50/50 [==============================] - 36s 723ms/step - loss: 0.2283 - acc: 0.9075 - val_loss: 4.0869 - val_acc: 0.5000\n",
      "163/163 [==============================] - 46s 283ms/step - loss: 1.6050 - acc: 0.7441\n",
      "\n",
      "Training Score: [1.605043888092041, 0.744056761264801]\n",
      "20/20 [==============================] - 5s 233ms/step - loss: 2.9550 - acc: 0.6250\n",
      "\n",
      "Test Score: [2.9550492763519287, 0.625]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "model = models.Sequential()\n",
    "model = Sequential()\n",
    "model.add(layers.Conv2D(32 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(layers.Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(layers.Conv2D(64 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(layers.Conv2D(128 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(layers.Conv2D(256 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(units = 128 , activation = 'relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(units = 1 , activation = 'sigmoid'))\n",
    "model.compile(optimizer = \"rmsprop\" , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_images,\n",
    "                    train_y,\n",
    "                    steps_per_epoch=50,\n",
    "                    epochs=20,\n",
    "                    batch_size=8,\n",
    "                    validation_data=(val_images, val_y))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nTraining Score: {model.evaluate(train_images, train_y)}\")\n",
    "print(f\"\\nTest Score: {model.evaluate(test_images, test_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 43s 262ms/step - loss: 1.6050 - acc: 0.7441\n",
      "\n",
      "Training Score: [1.605043888092041, 0.744056761264801]\n",
      "20/20 [==============================] - 5s 266ms/step - loss: 2.9550 - acc: 0.6250\n",
      "\n",
      "Test Score: [2.9550492763519287, 0.625]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\nTraining Score: {model.evaluate(train_images, train_y)}\")\n",
    "print(f\"\\nTest Score: {model.evaluate(test_images, test_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_classes = model.predict_classes(test_images, verbose = 0)\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    "recall_score(test_y, yhat_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 256, 256, 32)      320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 128, 64)      18496     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               2097280   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,524,353\n",
      "Trainable params: 2,523,265\n",
      "Non-trainable params: 1,088\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 254, 254, 16)      160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 127, 127, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 125, 125, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 60, 60, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,666,113\n",
      "Trainable params: 1,666,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Conv2D(filters=16, kernel_size=3, activation='relu', input_shape=(256,256,1)))\n",
    "\n",
    "model.add(layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(units=128, activation='relu'))\n",
    "model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "from os import walk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "163/163 [==============================] - 78s 476ms/step - loss: 1.5188 - accuracy: 0.7859 - val_loss: 1.6441 - val_accuracy: 0.5625\n",
      "Epoch 2/20\n",
      "163/163 [==============================] - 73s 446ms/step - loss: 0.3368 - accuracy: 0.8501 - val_loss: 1.6525 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "163/163 [==============================] - 72s 443ms/step - loss: 0.2670 - accuracy: 0.8875 - val_loss: 2.0742 - val_accuracy: 0.6875\n",
      "Epoch 4/20\n",
      "163/163 [==============================] - 77s 474ms/step - loss: 0.2312 - accuracy: 0.9078 - val_loss: 2.3992 - val_accuracy: 0.6250\n",
      "Epoch 5/20\n",
      "163/163 [==============================] - 78s 479ms/step - loss: 0.1958 - accuracy: 0.9195 - val_loss: 3.1725 - val_accuracy: 0.5625\n",
      "Epoch 6/20\n",
      "163/163 [==============================] - 80s 489ms/step - loss: 0.1636 - accuracy: 0.9344 - val_loss: 3.3090 - val_accuracy: 0.6250\n",
      "Epoch 7/20\n",
      "163/163 [==============================] - 83s 507ms/step - loss: 0.1639 - accuracy: 0.9323 - val_loss: 4.5276 - val_accuracy: 0.5625\n",
      "Epoch 8/20\n",
      "163/163 [==============================] - 80s 488ms/step - loss: 0.1321 - accuracy: 0.9461 - val_loss: 3.5782 - val_accuracy: 0.6250\n",
      "Epoch 9/20\n",
      "163/163 [==============================] - 71s 439ms/step - loss: 0.1158 - accuracy: 0.9513 - val_loss: 5.1139 - val_accuracy: 0.5000\n",
      "Epoch 10/20\n",
      "163/163 [==============================] - 75s 459ms/step - loss: 0.0988 - accuracy: 0.9594 - val_loss: 5.7063 - val_accuracy: 0.5000\n",
      "Epoch 11/20\n",
      "163/163 [==============================] - 73s 451ms/step - loss: 0.1003 - accuracy: 0.9638 - val_loss: 6.2497 - val_accuracy: 0.6250\n",
      "Epoch 12/20\n",
      "163/163 [==============================] - 74s 453ms/step - loss: 0.0880 - accuracy: 0.9661 - val_loss: 4.5430 - val_accuracy: 0.6250\n",
      "Epoch 13/20\n",
      "163/163 [==============================] - 73s 450ms/step - loss: 0.0816 - accuracy: 0.9680 - val_loss: 7.0089 - val_accuracy: 0.6250\n",
      "Epoch 14/20\n",
      "163/163 [==============================] - 75s 457ms/step - loss: 0.0667 - accuracy: 0.9776 - val_loss: 11.2077 - val_accuracy: 0.6250\n",
      "Epoch 15/20\n",
      "163/163 [==============================] - 75s 461ms/step - loss: 0.0472 - accuracy: 0.9824 - val_loss: 12.1163 - val_accuracy: 0.6250\n",
      "Epoch 16/20\n",
      "163/163 [==============================] - 75s 457ms/step - loss: 0.1076 - accuracy: 0.9634 - val_loss: 7.7925 - val_accuracy: 0.6250\n",
      "Epoch 17/20\n",
      "163/163 [==============================] - 75s 459ms/step - loss: 0.0580 - accuracy: 0.9778 - val_loss: 10.5883 - val_accuracy: 0.6250\n",
      "Epoch 18/20\n",
      "163/163 [==============================] - 75s 459ms/step - loss: 0.0497 - accuracy: 0.9814 - val_loss: 9.5896 - val_accuracy: 0.5625\n",
      "Epoch 19/20\n",
      "163/163 [==============================] - 75s 459ms/step - loss: 0.1284 - accuracy: 0.9532 - val_loss: 11.9291 - val_accuracy: 0.5625\n",
      "Epoch 20/20\n",
      "163/163 [==============================] - 75s 458ms/step - loss: 0.0725 - accuracy: 0.9747 - val_loss: 7.7764 - val_accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "             loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics = ['accuracy',])  \n",
    "\n",
    "history = model.fit(train_images, \n",
    "                    train_y,  \n",
    "                    epochs=20, \n",
    "                    validation_data = (val_images, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 18s 112ms/step - loss: 0.0338 - accuracy: 0.9896\n",
      "\n",
      "Training Score: [0.03382197767496109, 0.9896472096443176]\n",
      "20/20 [==============================] - 2s 104ms/step - loss: 2.1964 - accuracy: 0.7676\n",
      "\n",
      "Test Score: [2.1963613033294678, 0.7676281929016113]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTraining Score: {model.evaluate(train_images, train_y)}\")\n",
    "print(f\"\\nTest Score: {model.evaluate(test_images, test_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47435897435897434"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_classes = model.predict_classes(test_images, verbose = 0)\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    "recall_score(test_y, yhat_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "30/30 [==============================] - 69s 2s/step - loss: 0.7360 - accuracy: 0.7034 - val_loss: 1.6311 - val_accuracy: 0.6250\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 70s 2s/step - loss: 0.3776 - accuracy: 0.8184 - val_loss: 2.0271 - val_accuracy: 0.7500\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 70s 2s/step - loss: 0.3116 - accuracy: 0.8553 - val_loss: 2.3801 - val_accuracy: 0.6250\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 69s 2s/step - loss: 0.2919 - accuracy: 0.8698 - val_loss: 1.8069 - val_accuracy: 0.6250\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 69s 2s/step - loss: 0.2545 - accuracy: 0.8880 - val_loss: 2.3197 - val_accuracy: 0.6250\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 69s 2s/step - loss: 0.1968 - accuracy: 0.9187 - val_loss: 2.2417 - val_accuracy: 0.6250\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 71s 2s/step - loss: 0.1540 - accuracy: 0.9340 - val_loss: 3.8111 - val_accuracy: 0.6250\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 70s 2s/step - loss: 0.1304 - accuracy: 0.9473 - val_loss: 3.7260 - val_accuracy: 0.6875\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 69s 2s/step - loss: 0.0820 - accuracy: 0.9670 - val_loss: 3.0131 - val_accuracy: 0.6250\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 69s 2s/step - loss: 0.0818 - accuracy: 0.9689 - val_loss: 4.5301 - val_accuracy: 0.6875\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 70s 2s/step - loss: 0.0586 - accuracy: 0.9768 - val_loss: 6.8767 - val_accuracy: 0.6250\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 68s 2s/step - loss: 0.0686 - accuracy: 0.9749 - val_loss: 5.7791 - val_accuracy: 0.6875\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 66s 2s/step - loss: 0.0238 - accuracy: 0.9927 - val_loss: 8.3918 - val_accuracy: 0.6250\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 66s 2s/step - loss: 0.0342 - accuracy: 0.9895 - val_loss: 7.7771 - val_accuracy: 0.6250\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 66s 2s/step - loss: 0.0487 - accuracy: 0.9831 - val_loss: 8.3834 - val_accuracy: 0.6875\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 67s 2s/step - loss: 0.0316 - accuracy: 0.9900 - val_loss: 9.3445 - val_accuracy: 0.6250\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 66s 2s/step - loss: 0.0238 - accuracy: 0.9923 - val_loss: 10.1954 - val_accuracy: 0.6250\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 66s 2s/step - loss: 0.0111 - accuracy: 0.9965 - val_loss: 11.5224 - val_accuracy: 0.6875\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 66s 2s/step - loss: 0.0070 - accuracy: 0.9977 - val_loss: 13.8494 - val_accuracy: 0.6250\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 66s 2s/step - loss: 0.0148 - accuracy: 0.9956 - val_loss: 13.4448 - val_accuracy: 0.6250\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 256, 256, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 529,409\n",
      "Trainable params: 529,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "163/163 [==============================] - 19s 116ms/step - loss: 0.0086 - accuracy: 0.9973\n",
      "\n",
      "Training Score: [0.008639500476419926, 0.9973159432411194]\n",
      "20/20 [==============================] - 2s 108ms/step - loss: 2.0631 - accuracy: 0.8077\n",
      "\n",
      "Test Score: [2.0630970001220703, 0.807692289352417]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5598290598290598"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(filters = 32, strides = (1,1), kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters = 64, strides = (2,2), kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters = 128, strides = (2,2), kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters = 256, strides = (2,2), kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(layers.MaxPool2D((2,2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation = 'relu'))\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'BinaryCrossentropy', \n",
    "             optimizer = 'adam', \n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(train_images,\n",
    "                   train_y, \n",
    "                   steps_per_epoch = 30, \n",
    "                   epochs = 20,\n",
    "                   validation_data = (val_images, val_y))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nTraining Score: {model.evaluate(train_images, train_y)}\")\n",
    "print(f\"\\nTest Score: {model.evaluate(test_images, test_y)}\")\n",
    "\n",
    "yhat_classes = model.predict_classes(test_images, verbose = 0)\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    "recall_score(test_y, yhat_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
